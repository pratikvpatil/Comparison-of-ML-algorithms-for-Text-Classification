{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pratikp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "c:\\users\\pratikp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics Vidhya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "ENFJ     190\n",
       "ENFP     675\n",
       "ENTJ     231\n",
       "ENTP     685\n",
       "ESFJ      42\n",
       "ESFP      48\n",
       "ESTJ      39\n",
       "ESTP      89\n",
       "INFJ    1470\n",
       "INFP    1832\n",
       "INTJ    1091\n",
       "INTP    1304\n",
       "ISFJ     166\n",
       "ISFP     271\n",
       "ISTJ     205\n",
       "ISTP     337\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pandas.read_csv(\"mbti_1.csv\")\n",
    "\n",
    "#df = df.iloc[0:10,:]\n",
    "df.groupby('type').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8675"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['posts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = df['posts']\n",
    "trainDF['label'] = df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6506"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,5), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6506, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_tfidf.tocsc().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# usage of any word2vec, its format of 300 dimension and 1st element as word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999994\n",
      ",\n",
      "the\n",
      ".\n",
      "and\n",
      "of\n",
      "to\n",
      "in\n",
      "a\n",
      "\"\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "    print(values[0])\n",
    "    if i==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'the': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'and': 5,\n",
       " 'of': 6,\n",
       " 'you': 7,\n",
       " 'that': 8,\n",
       " 'it': 9,\n",
       " 'is': 10,\n",
       " 'in': 11,\n",
       " 'my': 12,\n",
       " 'but': 13,\n",
       " 'for': 14,\n",
       " 'have': 15,\n",
       " 'with': 16,\n",
       " 'me': 17,\n",
       " 'this': 18,\n",
       " \"i'm\": 19,\n",
       " 'be': 20,\n",
       " 'not': 21,\n",
       " 'are': 22,\n",
       " 'like': 23,\n",
       " 'on': 24,\n",
       " 'an': 25,\n",
       " 'as': 26,\n",
       " 'so': 27,\n",
       " 'was': 28,\n",
       " 'if': 29,\n",
       " 'just': 30,\n",
       " 'or': 31,\n",
       " 'do': 32,\n",
       " 'what': 33,\n",
       " 'about': 34,\n",
       " \"don't\": 35,\n",
       " 'think': 36,\n",
       " 'people': 37,\n",
       " 'your': 38,\n",
       " 'when': 39,\n",
       " \"it's\": 40,\n",
       " 'at': 41,\n",
       " 'can': 42,\n",
       " 'all': 43,\n",
       " 'know': 44,\n",
       " 'one': 45,\n",
       " 'they': 46,\n",
       " 'more': 47,\n",
       " 'really': 48,\n",
       " 'would': 49,\n",
       " 'we': 50,\n",
       " 'how': 51,\n",
       " 'out': 52,\n",
       " 'because': 53,\n",
       " 'get': 54,\n",
       " 'from': 55,\n",
       " 'am': 56,\n",
       " \"i've\": 57,\n",
       " 'some': 58,\n",
       " 'time': 59,\n",
       " 'he': 60,\n",
       " 'up': 61,\n",
       " 'very': 62,\n",
       " 'com': 63,\n",
       " 'there': 64,\n",
       " 'no': 65,\n",
       " 'them': 66,\n",
       " 'who': 67,\n",
       " 'feel': 68,\n",
       " 'much': 69,\n",
       " 'well': 70,\n",
       " 'being': 71,\n",
       " 'too': 72,\n",
       " 'been': 73,\n",
       " 'by': 74,\n",
       " 'love': 75,\n",
       " 'good': 76,\n",
       " 'things': 77,\n",
       " 'other': 78,\n",
       " 'say': 79,\n",
       " 'way': 80,\n",
       " 'something': 81,\n",
       " 'want': 82,\n",
       " 'watch': 83,\n",
       " 'see': 84,\n",
       " 'had': 85,\n",
       " 'most': 86,\n",
       " 'will': 87,\n",
       " 'only': 88,\n",
       " 'also': 89,\n",
       " 'www': 90,\n",
       " 'youtube': 91,\n",
       " 'then': 92,\n",
       " 'has': 93,\n",
       " 'http': 94,\n",
       " 'v': 95,\n",
       " 'type': 96,\n",
       " 'than': 97,\n",
       " 'now': 98,\n",
       " 'even': 99,\n",
       " 'always': 100,\n",
       " 'she': 101,\n",
       " 'lot': 102,\n",
       " 'make': 103,\n",
       " 'someone': 104,\n",
       " \"you're\": 105,\n",
       " 'why': 106,\n",
       " 'here': 107,\n",
       " 'their': 108,\n",
       " 'myself': 109,\n",
       " 'never': 110,\n",
       " 'life': 111,\n",
       " 'her': 112,\n",
       " 'could': 113,\n",
       " 'go': 114,\n",
       " 'any': 115,\n",
       " 'though': 116,\n",
       " \"that's\": 117,\n",
       " 'find': 118,\n",
       " 'thing': 119,\n",
       " 'into': 120,\n",
       " 'which': 121,\n",
       " 'actually': 122,\n",
       " \"can't\": 123,\n",
       " 'going': 124,\n",
       " 'right': 125,\n",
       " 'first': 126,\n",
       " 'sure': 127,\n",
       " 'infp': 128,\n",
       " 'him': 129,\n",
       " 'pretty': 130,\n",
       " 'person': 131,\n",
       " 'yes': 132,\n",
       " 'friends': 133,\n",
       " 'need': 134,\n",
       " 'same': 135,\n",
       " 'infj': 136,\n",
       " 'were': 137,\n",
       " 'still': 138,\n",
       " 'intj': 139,\n",
       " 'work': 140,\n",
       " 'https': 141,\n",
       " 'his': 142,\n",
       " '1': 143,\n",
       " 'thought': 144,\n",
       " 'said': 145,\n",
       " \"i'd\": 146,\n",
       " 'where': 147,\n",
       " 'many': 148,\n",
       " 'did': 149,\n",
       " 'got': 150,\n",
       " 'over': 151,\n",
       " 'sometimes': 152,\n",
       " 'intp': 153,\n",
       " 'read': 154,\n",
       " 'take': 155,\n",
       " 'friend': 156,\n",
       " 'probably': 157,\n",
       " 'maybe': 158,\n",
       " '2': 159,\n",
       " 'around': 160,\n",
       " 'thread': 161,\n",
       " 'best': 162,\n",
       " 'try': 163,\n",
       " 'off': 164,\n",
       " 'back': 165,\n",
       " 'those': 166,\n",
       " 'years': 167,\n",
       " 'mean': 168,\n",
       " 'should': 169,\n",
       " 'anything': 170,\n",
       " \"i'll\": 171,\n",
       " 'our': 172,\n",
       " 'does': 173,\n",
       " 'lol': 174,\n",
       " 'post': 175,\n",
       " \"didn't\": 176,\n",
       " 'little': 177,\n",
       " \"doesn't\": 178,\n",
       " 'kind': 179,\n",
       " 'while': 180,\n",
       " 'after': 181,\n",
       " '3': 182,\n",
       " 'long': 183,\n",
       " 'yeah': 184,\n",
       " 'few': 185,\n",
       " 'usually': 186,\n",
       " 'better': 187,\n",
       " 'us': 188,\n",
       " 'two': 189,\n",
       " 'enfp': 190,\n",
       " 'these': 191,\n",
       " 'look': 192,\n",
       " 'day': 193,\n",
       " 'different': 194,\n",
       " 'might': 195,\n",
       " 'before': 196,\n",
       " \"'\": 197,\n",
       " 'ever': 198,\n",
       " 'world': 199,\n",
       " 'seems': 200,\n",
       " 'its': 201,\n",
       " 'makes': 202,\n",
       " 'thanks': 203,\n",
       " 'understand': 204,\n",
       " 'since': 205,\n",
       " 'oh': 206,\n",
       " 'thinking': 207,\n",
       " 'others': 208,\n",
       " 'own': 209,\n",
       " 'may': 210,\n",
       " 'both': 211,\n",
       " 'hard': 212,\n",
       " 'agree': 213,\n",
       " 'trying': 214,\n",
       " 'quite': 215,\n",
       " 'bit': 216,\n",
       " 'through': 217,\n",
       " 'everyone': 218,\n",
       " 'mind': 219,\n",
       " 'point': 220,\n",
       " 'new': 221,\n",
       " 'great': 222,\n",
       " 'tell': 223,\n",
       " 'guess': 224,\n",
       " 'come': 225,\n",
       " 'made': 226,\n",
       " 'doing': 227,\n",
       " 'entp': 228,\n",
       " 'every': 229,\n",
       " 'school': 230,\n",
       " 'types': 231,\n",
       " 'feeling': 232,\n",
       " 'thank': 233,\n",
       " 'talk': 234,\n",
       " 'use': 235,\n",
       " 'down': 236,\n",
       " 'believe': 237,\n",
       " 'interesting': 238,\n",
       " 'definitely': 239,\n",
       " 'bad': 240,\n",
       " 'everything': 241,\n",
       " 'happy': 242,\n",
       " 'having': 243,\n",
       " 'give': 244,\n",
       " 'used': 245,\n",
       " 'often': 246,\n",
       " 'seem': 247,\n",
       " 'relationship': 248,\n",
       " 'help': 249,\n",
       " 'personality': 250,\n",
       " 'last': 251,\n",
       " 'guys': 252,\n",
       " 'times': 253,\n",
       " 'anyone': 254,\n",
       " 'question': 255,\n",
       " 'jpg': 256,\n",
       " 'another': 257,\n",
       " \"he's\": 258,\n",
       " \"there's\": 259,\n",
       " 'part': 260,\n",
       " 'enough': 261,\n",
       " 'idea': 262,\n",
       " 'hate': 263,\n",
       " 'p': 264,\n",
       " 'talking': 265,\n",
       " 'such': 266,\n",
       " 'getting': 267,\n",
       " 'true': 268,\n",
       " \"isn't\": 269,\n",
       " 'mbti': 270,\n",
       " 'sense': 271,\n",
       " 'least': 272,\n",
       " 'tend': 273,\n",
       " 'else': 274,\n",
       " 'keep': 275,\n",
       " 'put': 276,\n",
       " 'nice': 277,\n",
       " 'once': 278,\n",
       " 'd': 279,\n",
       " 'self': 280,\n",
       " 'sounds': 281,\n",
       " 'either': 282,\n",
       " 'care': 283,\n",
       " 'year': 284,\n",
       " 'problem': 285,\n",
       " 'sorry': 286,\n",
       " 'nothing': 287,\n",
       " 'without': 288,\n",
       " 'music': 289,\n",
       " 'again': 290,\n",
       " 'stuff': 291,\n",
       " 'haha': 292,\n",
       " 'high': 293,\n",
       " 'found': 294,\n",
       " 'yourself': 295,\n",
       " 'reading': 296,\n",
       " 'fe': 297,\n",
       " 'let': 298,\n",
       " '5': 299,\n",
       " 'wrong': 300,\n",
       " 'fun': 301,\n",
       " 'especially': 302,\n",
       " 'welcome': 303,\n",
       " \"they're\": 304,\n",
       " 'fi': 305,\n",
       " 'experience': 306,\n",
       " 'guy': 307,\n",
       " 'between': 308,\n",
       " 'reason': 309,\n",
       " 'forum': 310,\n",
       " '4': 311,\n",
       " 'however': 312,\n",
       " 'start': 313,\n",
       " 'far': 314,\n",
       " 'ask': 315,\n",
       " 'rather': 316,\n",
       " 'real': 317,\n",
       " 'social': 318,\n",
       " 'feelings': 319,\n",
       " 'test': 320,\n",
       " 'looking': 321,\n",
       " 'istj': 322,\n",
       " 'saying': 323,\n",
       " 'live': 324,\n",
       " 'done': 325,\n",
       " 'exactly': 326,\n",
       " 'almost': 327,\n",
       " 'enjoy': 328,\n",
       " 'away': 329,\n",
       " 'each': 330,\n",
       " 'old': 331,\n",
       " 'close': 332,\n",
       " 'remember': 333,\n",
       " 'istp': 334,\n",
       " 'end': 335,\n",
       " 'man': 336,\n",
       " 'ago': 337,\n",
       " \"wouldn't\": 338,\n",
       " 'comes': 339,\n",
       " 'big': 340,\n",
       " 'fact': 341,\n",
       " 'yet': 342,\n",
       " 'ne': 343,\n",
       " 'already': 344,\n",
       " 'ni': 345,\n",
       " 'answer': 346,\n",
       " 'relate': 347,\n",
       " 'entj': 348,\n",
       " 'wanted': 349,\n",
       " 'enfj': 350,\n",
       " 'making': 351,\n",
       " \"haven't\": 352,\n",
       " 'place': 353,\n",
       " 'girl': 354,\n",
       " 'isfp': 355,\n",
       " 'head': 356,\n",
       " 'able': 357,\n",
       " 'less': 358,\n",
       " 'functions': 359,\n",
       " 'family': 360,\n",
       " 'laughing': 361,\n",
       " 'im': 362,\n",
       " 'change': 363,\n",
       " 'whole': 364,\n",
       " 'until': 365,\n",
       " 'alone': 366,\n",
       " 'told': 367,\n",
       " 'isfj': 368,\n",
       " 'hope': 369,\n",
       " \"we're\": 370,\n",
       " 'completely': 371,\n",
       " 'job': 372,\n",
       " 'started': 373,\n",
       " 'interested': 374,\n",
       " 'cool': 375,\n",
       " 'using': 376,\n",
       " 'show': 377,\n",
       " 'although': 378,\n",
       " 'words': 379,\n",
       " \"she's\": 380,\n",
       " 'seen': 381,\n",
       " 'ti': 382,\n",
       " 'past': 383,\n",
       " 'today': 384,\n",
       " 'situation': 385,\n",
       " 'certain': 386,\n",
       " 'wish': 387,\n",
       " 'sort': 388,\n",
       " 'play': 389,\n",
       " 'days': 390,\n",
       " 'infps': 391,\n",
       " 'etc': 392,\n",
       " 'felt': 393,\n",
       " 'general': 394,\n",
       " 'crazy': 395,\n",
       " 'emotional': 396,\n",
       " 'funny': 397,\n",
       " 'along': 398,\n",
       " \"wasn't\": 399,\n",
       " 'stop': 400,\n",
       " 'thoughts': 401,\n",
       " 'hey': 402,\n",
       " 'mostly': 403,\n",
       " 'course': 404,\n",
       " 'based': 405,\n",
       " 'god': 406,\n",
       " 'become': 407,\n",
       " 'took': 408,\n",
       " 'must': 409,\n",
       " 'personally': 410,\n",
       " 'emotions': 411,\n",
       " 'book': 412,\n",
       " 'awesome': 413,\n",
       " 'please': 414,\n",
       " 'example': 415,\n",
       " 'totally': 416,\n",
       " 'similar': 417,\n",
       " 'estp': 418,\n",
       " 'personal': 419,\n",
       " 'went': 420,\n",
       " 'weird': 421,\n",
       " 'se': 422,\n",
       " 'ones': 423,\n",
       " 'met': 424,\n",
       " 'likely': 425,\n",
       " 'means': 426,\n",
       " \"aren't\": 427,\n",
       " 'intjs': 428,\n",
       " 'sound': 429,\n",
       " 'ok': 430,\n",
       " 'esfj': 431,\n",
       " 'night': 432,\n",
       " 'strong': 433,\n",
       " 'hear': 434,\n",
       " 'hi': 435,\n",
       " 'open': 436,\n",
       " 'questions': 437,\n",
       " 'favorite': 438,\n",
       " 'important': 439,\n",
       " '6': 440,\n",
       " 'okay': 441,\n",
       " 'function': 442,\n",
       " 'matter': 443,\n",
       " 'advice': 444,\n",
       " 'dont': 445,\n",
       " 'home': 446,\n",
       " 'recently': 447,\n",
       " 'possible': 448,\n",
       " 'learn': 449,\n",
       " 's': 450,\n",
       " 'name': 451,\n",
       " \"won't\": 452,\n",
       " 'infjs': 453,\n",
       " 'working': 454,\n",
       " 'whatever': 455,\n",
       " '10': 456,\n",
       " 'te': 457,\n",
       " 'side': 458,\n",
       " 'song': 459,\n",
       " 'came': 460,\n",
       " 'call': 461,\n",
       " 'esfp': 462,\n",
       " 'anyway': 463,\n",
       " \"what's\": 464,\n",
       " 'women': 465,\n",
       " 'tongue': 466,\n",
       " 'wow': 467,\n",
       " 'mom': 468,\n",
       " 'xd': 469,\n",
       " 'together': 470,\n",
       " 'months': 471,\n",
       " \"you've\": 472,\n",
       " 'video': 473,\n",
       " 'couple': 474,\n",
       " 'taking': 475,\n",
       " 'writing': 476,\n",
       " 'mine': 477,\n",
       " 'female': 478,\n",
       " 'generally': 479,\n",
       " 'whether': 480,\n",
       " 'face': 481,\n",
       " 'seriously': 482,\n",
       " 'depends': 483,\n",
       " 'write': 484,\n",
       " 'case': 485,\n",
       " 'word': 486,\n",
       " 'easy': 487,\n",
       " 'honestly': 488,\n",
       " 'e': 489,\n",
       " 'next': 490,\n",
       " 'sad': 491,\n",
       " 'unless': 492,\n",
       " 'intps': 493,\n",
       " 'second': 494,\n",
       " 'relationships': 495,\n",
       " 'hello': 496,\n",
       " 'consider': 497,\n",
       " 'tried': 498,\n",
       " 'shit': 499,\n",
       " 'small': 500,\n",
       " 't': 501,\n",
       " 'moment': 502,\n",
       " 'si': 503,\n",
       " 'gets': 504,\n",
       " 'perhaps': 505,\n",
       " 'ideas': 506,\n",
       " 'human': 507,\n",
       " 'college': 508,\n",
       " 'conversation': 509,\n",
       " 'share': 510,\n",
       " 'sex': 511,\n",
       " 'movie': 512,\n",
       " 'called': 513,\n",
       " 'game': 514,\n",
       " 'group': 515,\n",
       " 'age': 516,\n",
       " 'heard': 517,\n",
       " 'says': 518,\n",
       " 'free': 519,\n",
       " 'prefer': 520,\n",
       " 'opinion': 521,\n",
       " 'absolutely': 522,\n",
       " 'story': 523,\n",
       " 'list': 524,\n",
       " 'o': 525,\n",
       " 'instead': 526,\n",
       " 'male': 527,\n",
       " 'estj': 528,\n",
       " 'common': 529,\n",
       " 'left': 530,\n",
       " 'looks': 531,\n",
       " 'feels': 532,\n",
       " 'saw': 533,\n",
       " 'goes': 534,\n",
       " 'meet': 535,\n",
       " '7': 536,\n",
       " 'enneagram': 537,\n",
       " 'mother': 538,\n",
       " 'eyes': 539,\n",
       " 'posts': 540,\n",
       " 'difficult': 541,\n",
       " 'introverted': 542,\n",
       " 'n': 543,\n",
       " 'heart': 544,\n",
       " 'basically': 545,\n",
       " 'sleep': 546,\n",
       " '9': 547,\n",
       " 'problems': 548,\n",
       " 'kinda': 549,\n",
       " 'money': 550,\n",
       " '8': 551,\n",
       " 'house': 552,\n",
       " 'deal': 553,\n",
       " 'dad': 554,\n",
       " 'figure': 555,\n",
       " 'listen': 556,\n",
       " 'happen': 557,\n",
       " 'happens': 558,\n",
       " 'books': 559,\n",
       " 'wonder': 560,\n",
       " 'watching': 561,\n",
       " 'simply': 562,\n",
       " 'asking': 563,\n",
       " 'explain': 564,\n",
       " 'curious': 565,\n",
       " 'easily': 566,\n",
       " 'asked': 567,\n",
       " 'parents': 568,\n",
       " 'extremely': 569,\n",
       " 'act': 570,\n",
       " 'stupid': 571,\n",
       " 'deep': 572,\n",
       " 'media': 573,\n",
       " 'fit': 574,\n",
       " 'week': 575,\n",
       " 'interest': 576,\n",
       " 'themselves': 577,\n",
       " 'coming': 578,\n",
       " 'ways': 579,\n",
       " 'three': 580,\n",
       " 'honest': 581,\n",
       " 'realize': 582,\n",
       " 'information': 583,\n",
       " 'topic': 584,\n",
       " 'f': 585,\n",
       " 'seeing': 586,\n",
       " 'order': 587,\n",
       " 'games': 588,\n",
       " 'tumblr': 589,\n",
       " \"couldn't\": 590,\n",
       " 'serious': 591,\n",
       " 'hours': 592,\n",
       " 'dear': 593,\n",
       " 'men': 594,\n",
       " 'class': 595,\n",
       " 'knew': 596,\n",
       " 'towards': 597,\n",
       " 'speak': 598,\n",
       " 'child': 599,\n",
       " 'living': 600,\n",
       " 'attention': 601,\n",
       " 'hurt': 602,\n",
       " 'stay': 603,\n",
       " 'full': 604,\n",
       " 'posted': 605,\n",
       " 'theory': 606,\n",
       " 'short': 607,\n",
       " 'half': 608,\n",
       " 'beautiful': 609,\n",
       " 'understanding': 610,\n",
       " 'room': 611,\n",
       " 'level': 612,\n",
       " 'needs': 613,\n",
       " 'gif': 614,\n",
       " 'hmm': 615,\n",
       " 'hell': 616,\n",
       " 'happened': 617,\n",
       " 'cause': 618,\n",
       " 'sent': 619,\n",
       " 'works': 620,\n",
       " 'enfps': 621,\n",
       " 'art': 622,\n",
       " 'playing': 623,\n",
       " 'noticed': 624,\n",
       " 'currently': 625,\n",
       " 'fine': 626,\n",
       " 'meant': 627,\n",
       " 'lost': 628,\n",
       " 'trust': 629,\n",
       " 'spend': 630,\n",
       " 'picture': 631,\n",
       " 'except': 632,\n",
       " 'appreciate': 633,\n",
       " 'perfect': 634,\n",
       " 'sister': 635,\n",
       " 'internet': 636,\n",
       " 'random': 637,\n",
       " 'dating': 638,\n",
       " 'super': 639,\n",
       " 'amazing': 640,\n",
       " 'during': 641,\n",
       " 'dream': 642,\n",
       " 'known': 643,\n",
       " 'proud': 644,\n",
       " '100': 645,\n",
       " 'under': 646,\n",
       " 'liked': 647,\n",
       " 'girls': 648,\n",
       " 'entps': 649,\n",
       " 'online': 650,\n",
       " 'move': 651,\n",
       " 'glad': 652,\n",
       " 'leave': 653,\n",
       " \"you'll\": 654,\n",
       " 'loved': 655,\n",
       " 'hand': 656,\n",
       " 'brain': 657,\n",
       " 'outside': 658,\n",
       " 'cognitive': 659,\n",
       " 'given': 660,\n",
       " 'due': 661,\n",
       " 'lack': 662,\n",
       " 'related': 663,\n",
       " 'add': 664,\n",
       " 'wait': 665,\n",
       " 'brother': 666,\n",
       " 'imagine': 667,\n",
       " 'physical': 668,\n",
       " 'wants': 669,\n",
       " 'fear': 670,\n",
       " 'learning': 671,\n",
       " 'against': 672,\n",
       " 'body': 673,\n",
       " 'difference': 674,\n",
       " 'view': 675,\n",
       " 'cannot': 676,\n",
       " 'truth': 677,\n",
       " 'takes': 678,\n",
       " 'issues': 679,\n",
       " 'angry': 680,\n",
       " 'taken': 681,\n",
       " 'dark': 682,\n",
       " 'future': 683,\n",
       " 'kids': 684,\n",
       " 'issue': 685,\n",
       " 'wondering': 686,\n",
       " 'black': 687,\n",
       " 'science': 688,\n",
       " \"here's\": 689,\n",
       " 'lots': 690,\n",
       " 'eat': 691,\n",
       " 'food': 692,\n",
       " 'nature': 693,\n",
       " 'later': 694,\n",
       " 'turn': 695,\n",
       " 'run': 696,\n",
       " 'reasons': 697,\n",
       " 'death': 698,\n",
       " 'above': 699,\n",
       " 'kid': 700,\n",
       " 'response': 701,\n",
       " 'fall': 702,\n",
       " 'woman': 703,\n",
       " 'confused': 704,\n",
       " 'character': 705,\n",
       " 'single': 706,\n",
       " 'control': 707,\n",
       " 'learned': 708,\n",
       " 'younger': 709,\n",
       " 'late': 710,\n",
       " 'u': 711,\n",
       " 'energy': 712,\n",
       " 'several': 713,\n",
       " 'quiet': 714,\n",
       " 'listening': 715,\n",
       " 'normal': 716,\n",
       " 'young': 717,\n",
       " 'inside': 718,\n",
       " 'value': 719,\n",
       " 'older': 720,\n",
       " 'top': 721,\n",
       " 'simple': 722,\n",
       " 'language': 723,\n",
       " 'stand': 724,\n",
       " 'reality': 725,\n",
       " 'wink': 726,\n",
       " 'suppose': 727,\n",
       " 'j': 728,\n",
       " 'movies': 729,\n",
       " 'laugh': 730,\n",
       " 'english': 731,\n",
       " 'non': 732,\n",
       " 'clear': 733,\n",
       " 'set': 734,\n",
       " 'process': 735,\n",
       " 'tapatalk': 736,\n",
       " 'sp': 737,\n",
       " 'sx': 738,\n",
       " 'cold': 739,\n",
       " 'quote': 740,\n",
       " 'logic': 741,\n",
       " 'hair': 742,\n",
       " 'certainly': 743,\n",
       " 'meaning': 744,\n",
       " 'date': 745,\n",
       " 'tests': 746,\n",
       " 'aware': 747,\n",
       " 'gonna': 748,\n",
       " 'truly': 749,\n",
       " 'comfortable': 750,\n",
       " 'fuck': 751,\n",
       " 'giving': 752,\n",
       " 'decided': 753,\n",
       " 'romantic': 754,\n",
       " 'bored': 755,\n",
       " 'telling': 756,\n",
       " 'description': 757,\n",
       " 'plan': 758,\n",
       " 'perc': 759,\n",
       " 'focus': 760,\n",
       " 'worth': 761,\n",
       " 'cute': 762,\n",
       " 'huge': 763,\n",
       " \"'i\": 764,\n",
       " 'afraid': 765,\n",
       " 'break': 766,\n",
       " 'dominant': 767,\n",
       " 'across': 768,\n",
       " 'constantly': 769,\n",
       " 'choose': 770,\n",
       " 'situations': 771,\n",
       " 'boyfriend': 772,\n",
       " 'state': 773,\n",
       " 'doubt': 774,\n",
       " 'system': 775,\n",
       " 'mentioned': 776,\n",
       " 'notice': 777,\n",
       " 'society': 778,\n",
       " 'anxiety': 779,\n",
       " 'respect': 780,\n",
       " 'damn': 781,\n",
       " 'lately': 782,\n",
       " 'obviously': 783,\n",
       " \"let's\": 784,\n",
       " 'results': 785,\n",
       " 'form': 786,\n",
       " 'experiences': 787,\n",
       " 'early': 788,\n",
       " 'finally': 789,\n",
       " 'finding': 790,\n",
       " 'anymore': 791,\n",
       " 'eye': 792,\n",
       " 'number': 793,\n",
       " 'math': 794,\n",
       " 'dreams': 795,\n",
       " 'smile': 796,\n",
       " 'boring': 797,\n",
       " '0': 798,\n",
       " 'low': 799,\n",
       " 'check': 800,\n",
       " 'speaking': 801,\n",
       " 'typing': 802,\n",
       " 'likes': 803,\n",
       " 'whenever': 804,\n",
       " 'particular': 805,\n",
       " 'rest': 806,\n",
       " 'specific': 807,\n",
       " 'negative': 808,\n",
       " 'space': 809,\n",
       " 'weeks': 810,\n",
       " 'term': 811,\n",
       " 'pick': 812,\n",
       " 'characters': 813,\n",
       " 'avoid': 814,\n",
       " 'main': 815,\n",
       " 'knowing': 816,\n",
       " 'somewhere': 817,\n",
       " 'power': 818,\n",
       " 'middle': 819,\n",
       " 'starting': 820,\n",
       " '20': 821,\n",
       " 'logical': 822,\n",
       " 'describe': 823,\n",
       " 'realized': 824,\n",
       " 'bring': 825,\n",
       " 'father': 826,\n",
       " 'points': 827,\n",
       " 'worry': 828,\n",
       " 'op': 829,\n",
       " 'knows': 830,\n",
       " 'computer': 831,\n",
       " 'helps': 832,\n",
       " 'phone': 833,\n",
       " 'knowledge': 834,\n",
       " 'images': 835,\n",
       " 'reply': 836,\n",
       " 'considered': 837,\n",
       " 'gave': 838,\n",
       " 'hold': 839,\n",
       " 'awkward': 840,\n",
       " 'b': 841,\n",
       " 'behind': 842,\n",
       " 'forget': 843,\n",
       " 'opposite': 844,\n",
       " 'mood': 845,\n",
       " 'annoying': 846,\n",
       " 'light': 847,\n",
       " 'follow': 848,\n",
       " 'introvert': 849,\n",
       " 'within': 850,\n",
       " 'sensitive': 851,\n",
       " 'pain': 852,\n",
       " 'forums': 853,\n",
       " 'line': 854,\n",
       " 'natural': 855,\n",
       " 'mental': 856,\n",
       " 'major': 857,\n",
       " 'literally': 858,\n",
       " 'easier': 859,\n",
       " 'cry': 860,\n",
       " 'strange': 861,\n",
       " 'ability': 862,\n",
       " 'somehow': 863,\n",
       " 'accurate': 864,\n",
       " 'shy': 865,\n",
       " 'extroverted': 866,\n",
       " 'tired': 867,\n",
       " 'feature': 868,\n",
       " 'soon': 869,\n",
       " 'style': 870,\n",
       " 'content': 871,\n",
       " 'terms': 872,\n",
       " 'trouble': 873,\n",
       " 'party': 874,\n",
       " 'children': 875,\n",
       " 'somewhat': 876,\n",
       " 'ex': 877,\n",
       " 'supposed': 878,\n",
       " 'values': 879,\n",
       " 'emotionally': 880,\n",
       " 'edit': 881,\n",
       " 'walk': 882,\n",
       " 'thats': 883,\n",
       " 'site': 884,\n",
       " 'hit': 885,\n",
       " 'necessarily': 886,\n",
       " 'entire': 887,\n",
       " 'behavior': 888,\n",
       " 'particularly': 889,\n",
       " 'otherwise': 890,\n",
       " 'white': 891,\n",
       " 'wrote': 892,\n",
       " 'study': 893,\n",
       " 'subject': 894,\n",
       " 'watched': 895,\n",
       " 'depression': 896,\n",
       " 'obvious': 897,\n",
       " 'vs': 898,\n",
       " 'special': 899,\n",
       " 'perspective': 900,\n",
       " 'x': 901,\n",
       " 'possibly': 902,\n",
       " 'answers': 903,\n",
       " 'wear': 904,\n",
       " 'healthy': 905,\n",
       " 'fucking': 906,\n",
       " 'gone': 907,\n",
       " 'worst': 908,\n",
       " 'attracted': 909,\n",
       " 'dom': 910,\n",
       " 'smart': 911,\n",
       " 'research': 912,\n",
       " 'intelligence': 913,\n",
       " 'ah': 914,\n",
       " 'gotten': 915,\n",
       " 'rarely': 916,\n",
       " 'straight': 917,\n",
       " 'worked': 918,\n",
       " 'apparently': 919,\n",
       " 'shows': 920,\n",
       " 'contact': 921,\n",
       " 'positive': 922,\n",
       " 'fellow': 923,\n",
       " 'bed': 924,\n",
       " 'songs': 925,\n",
       " 'posting': 926,\n",
       " 'net': 927,\n",
       " 'youtu': 928,\n",
       " 'amount': 929,\n",
       " 'religion': 930,\n",
       " 'sweet': 931,\n",
       " 'chance': 932,\n",
       " 'choice': 933,\n",
       " 'morning': 934,\n",
       " 'inferior': 935,\n",
       " 'argument': 936,\n",
       " 'threads': 937,\n",
       " 'needed': 938,\n",
       " 'married': 939,\n",
       " 'degree': 940,\n",
       " 'die': 941,\n",
       " 'expect': 942,\n",
       " 'blue': 943,\n",
       " 'fairly': 944,\n",
       " 'intuition': 945,\n",
       " 'country': 946,\n",
       " 'dislike': 947,\n",
       " 'looked': 948,\n",
       " 'hot': 949,\n",
       " 'correct': 950,\n",
       " 'touch': 951,\n",
       " 'cat': 952,\n",
       " 'link': 953,\n",
       " 'traits': 954,\n",
       " 'comment': 955,\n",
       " 'avatar': 956,\n",
       " 'gives': 957,\n",
       " 'tv': 958,\n",
       " 'month': 959,\n",
       " 'accept': 960,\n",
       " 'public': 961,\n",
       " 'c': 962,\n",
       " 'voice': 963,\n",
       " 'sit': 964,\n",
       " 'highly': 965,\n",
       " 'depressed': 966,\n",
       " 'original': 967,\n",
       " 'stress': 968,\n",
       " 'note': 969,\n",
       " 'evil': 970,\n",
       " 'partner': 971,\n",
       " 'creative': 972,\n",
       " 'hug': 973,\n",
       " 'clearly': 974,\n",
       " 'thinks': 975,\n",
       " 'itself': 976,\n",
       " 'conversations': 977,\n",
       " 'series': 978,\n",
       " 'lives': 979,\n",
       " 'express': 980,\n",
       " 'spent': 981,\n",
       " 'four': 982,\n",
       " 'career': 983,\n",
       " 'current': 984,\n",
       " 'unfortunately': 985,\n",
       " 'humor': 986,\n",
       " 'assume': 987,\n",
       " 'result': 988,\n",
       " 'intuitive': 989,\n",
       " \"people's\": 990,\n",
       " 'miss': 991,\n",
       " 'history': 992,\n",
       " 'car': 993,\n",
       " 'terrible': 994,\n",
       " 'large': 995,\n",
       " 'seemed': 996,\n",
       " 'changed': 997,\n",
       " 'hang': 998,\n",
       " 'written': 999,\n",
       " 'facebook': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      0,      0, ..., 146904, 146905,    197])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq_x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained word-embedding vectors \n",
    "embeddings_index = {}\n",
    "for i, line in enumerate(open('wiki-news-300d-1M.vec', encoding=\"utf8\")):\n",
    "    values = line.split()\n",
    "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
    "\n",
    "# create a tokenizer \n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=2000)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=2000)\n",
    "\n",
    "# create token-embedding mapping\n",
    "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "trainDF['decimal_or_digit_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isdecimal() or wrd.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun_count Done\n",
      "verb_count Done\n",
      "adj_count Done\n",
      "adv_count Done\n",
      "pron_count Done\n",
      "positive_count Done\n",
      "negative_count Done\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2a141ada4ada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'negative_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnegative_polarity_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"negative_count Done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'positive_by_negative'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcount_polarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"positive_by_negative Done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subjective_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msubjectivity_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pratikp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2549\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2551\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-2a141ada4ada>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'negative_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnegative_polarity_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"negative_count Done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'positive_by_negative'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcount_polarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"positive_by_negative Done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subjective_count'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msubjectivity_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-2a141ada4ada>\u001b[0m in \u001b[0;36mcount_polarity\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpositive\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "def count_polarity(x):\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for sentence in wiki.sentences:\n",
    "            if sentence.sentiment.polarity>0:\n",
    "                positive += 1\n",
    "            else:\n",
    "                negative += 1\n",
    "           \n",
    "    except:\n",
    "        pass\n",
    "    return positive/negative\n",
    "\n",
    "\n",
    "def positive_polarity_count(x):\n",
    "    positive = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for sentence in wiki.sentences:\n",
    "            if sentence.sentiment.polarity>0.3:\n",
    "                positive += 1           \n",
    "    except:\n",
    "        pass\n",
    "    return positive\n",
    "\n",
    "def negative_polarity_count(x):\n",
    "    negative = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for sentence in wiki.sentences:\n",
    "            if sentence.sentiment.polarity<0.3:\n",
    "                negative += 1           \n",
    "    except:\n",
    "        pass\n",
    "    return negative\n",
    "\n",
    "def subjectivity_count(x):\n",
    "    subjective_count = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for sentence in wiki.sentences:\n",
    "            if sentence.sentiment.subjectivity>0.7:\n",
    "                subjective_count += 1           \n",
    "    except:\n",
    "        pass\n",
    "    return subjective_count\n",
    "\n",
    "\n",
    "def objectivity_count(x):\n",
    "    objective_count = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for sentence in wiki.sentences:\n",
    "            if sentence.sentiment.subjectivity<0.3:\n",
    "                objective_count += 1           \n",
    "    except:\n",
    "        pass\n",
    "    return objective_count\n",
    "\n",
    "\n",
    "trainDF['noun_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "print(\"noun_count Done\")\n",
    "trainDF['verb_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "print(\"verb_count Done\")\n",
    "trainDF['adj_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "print(\"adj_count Done\")\n",
    "trainDF['adv_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "print(\"adv_count Done\")\n",
    "trainDF['pron_count'] = trainDF['text'].apply(lambda x: check_pos_tag(x, 'pron'))\n",
    "print(\"pron_count Done\")\n",
    "trainDF['positive_count'] = trainDF['text'].apply(lambda x: positive_polarity_count(x))\n",
    "print(\"positive_count Done\")\n",
    "trainDF['negative_count'] = trainDF['text'].apply(lambda x: negative_polarity_count(x))\n",
    "print(\"negative_count Done\")\n",
    "# trainDF['positive_by_negative'] = trainDF['text'].apply(lambda x: count_polarity(x))\n",
    "# print(\"positive_by_negative Done\")\n",
    "trainDF['subjective_count'] = trainDF['text'].apply(lambda x: subjectivity_count(x))\n",
    "print(\"subjective_count Done\")\n",
    "trainDF['objective_count'] = trainDF['text'].apply(lambda x: objectivity_count(x))\n",
    "print(\"objective_count Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subjective_count Done\n",
      "objective_count Done\n"
     ]
    }
   ],
   "source": [
    "trainDF['subjective_count'] = trainDF['text'].apply(lambda x: subjectivity_count(x))\n",
    "print(\"subjective_count Done\")\n",
    "trainDF['objective_count'] = trainDF['text'].apply(lambda x: objectivity_count(x))\n",
    "print(\"objective_count Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainDF.to_pickle(\"trainDF_final_after_POS_tagging.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# trainDF = pd.read_pickle(\"trainDF_final_after_POS_tagging.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid,valid_y, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    if is_neural_net:\n",
    "        classifier.fit(feature_vector_train, label,epochs=50)\n",
    "    else:\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on NLP based features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pratikp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, ALLL NLP Based Features:  0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pratikp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "X = trainDF.iloc[:,2:]\n",
    "y = trainDF.iloc[:,1:2]\n",
    "\n",
    "nlp_train_x, nlp_valid_x, nlp_train_y, nlp_valid_y = X.iloc[0:7,:], X.iloc[7:10,:], y.iloc[0:7,:], y.iloc[7:10,:]\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "nlp_train_y = encoder.fit_transform(nlp_train_y)\n",
    "nlp_valid_y = encoder.fit_transform(nlp_valid_y)\n",
    "\n",
    "accuracy = train_model(xgboost.XGBClassifier(), nlp_train_x, nlp_train_y, nlp_valid_x, nlp_valid_y)\n",
    "print(\"Xgb, ALLL NLP Based Features: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Scoring begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "algorithm = [BaggingClassifier()\n",
    ", AdaBoostClassifier()\n",
    ", ExtraTreesClassifier()\n",
    ", SVC(kernel='rbf')\n",
    ", GradientBoostingClassifier()\n",
    ", RandomForestClassifier()\n",
    ", PassiveAggressiveClassifier()\n",
    ", MultinomialNB()\n",
    ", RidgeClassifier()\n",
    ", RidgeClassifierCV()\n",
    ", SGDClassifier()\n",
    ", BernoulliNB()\n",
    ", KNeighborsClassifier()\n",
    ", XGBClassifier()\n",
    ", MLPClassifier()\n",
    ", LogisticRegression()\n",
    ", DecisionTreeClassifier()\n",
    ", ExtraTreeClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_meta_score(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    if is_neural_net:\n",
    "        classifier.fit(feature_vector_train, label,epochs=5)\n",
    "    else:\n",
    "        classifier.fit(feature_vector_train, label)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    train_meta_score = classifier.predict(feature_vector_train)\n",
    "    valid_meta_score = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        train_meta_score = train_meta_score.argmax(axis=-1)\n",
    "        valid_meta_score = valid_meta_score.argmax(axis=-1)\n",
    "        \n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions_on_test = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions_on_test = predictions_on_test.argmax(axis=-1)\n",
    "        \n",
    "    accuracy_on_test = metrics.accuracy_score(predictions_on_test, valid_y)\n",
    "\n",
    "    \n",
    "    return train_meta_score,valid_meta_score,accuracy_on_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = [xtrain_count, xtrain_tfidf, xtrain_tfidf_ngram, xtrain_tfidf_ngram_chars]\n",
    "X_VALID = [xvalid_count, xvalid_tfidf, xvalid_tfidf_ngram, xvalid_tfidf_ngram_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pratikp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "c:\\users\\pratikp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "c:\\users\\pratikp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "c:\\users\\pratikp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "c:\\users\\pratikp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "X_TRAIN_META = []\n",
    "X_VALID_META = []\n",
    "# Accuracy on test/validation set\n",
    "accuracy_on_test = []\n",
    "for alg in algorithm:\n",
    "    for i in range(len(X_TRAIN)):\n",
    "        x_train_meta, x_valid_meta,accuracy = return_meta_score(alg, X_TRAIN[i], train_y, X_VALID[i])\n",
    "        X_TRAIN_META.append(x_train_meta)\n",
    "        X_VALID_META.append(x_valid_meta)\n",
    "        accuracy_on_test.append([alg,i,accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "           bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False), 0.0],\n",
       " [BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "           bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False), 0.0],\n",
       " [BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "           bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False), 0.0],\n",
       " [BaggingClassifier(base_estimator=None, bootstrap=True,\n",
       "           bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False), 0.3333333333333333],\n",
       " [AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "            learning_rate=1.0, n_estimators=50, random_state=None), 0.0],\n",
       " [AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "            learning_rate=1.0, n_estimators=50, random_state=None), 0.0],\n",
       " [AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "            learning_rate=1.0, n_estimators=50, random_state=None), 0.0],\n",
       " [AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "            learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "  0.3333333333333333],\n",
       " [ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "  0.0],\n",
       " [ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "  0.0],\n",
       " [ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "  0.0],\n",
       " [ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "  0.0],\n",
       " [SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.0],\n",
       " [SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.0],\n",
       " [SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.0],\n",
       " [SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False), 0.0],\n",
       " [GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "                warm_start=False), 0.0],\n",
       " [GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "                warm_start=False), 0.0],\n",
       " [GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "                warm_start=False), 0.0],\n",
       " [GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "                warm_start=False), 0.0],\n",
       " [RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False), 0.0],\n",
       " [RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False), 0.0],\n",
       " [RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False), 0.0],\n",
       " [RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "              max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "              oob_score=False, random_state=None, verbose=0,\n",
       "              warm_start=False), 0.0],\n",
       " [PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
       "                fit_intercept=True, loss='hinge', max_iter=None, n_iter=None,\n",
       "                n_jobs=1, random_state=None, shuffle=True, tol=None,\n",
       "                verbose=0, warm_start=False), 0.0],\n",
       " [PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
       "                fit_intercept=True, loss='hinge', max_iter=None, n_iter=None,\n",
       "                n_jobs=1, random_state=None, shuffle=True, tol=None,\n",
       "                verbose=0, warm_start=False), 0.0],\n",
       " [PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
       "                fit_intercept=True, loss='hinge', max_iter=None, n_iter=None,\n",
       "                n_jobs=1, random_state=None, shuffle=True, tol=None,\n",
       "                verbose=0, warm_start=False), 0.0],\n",
       " [PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
       "                fit_intercept=True, loss='hinge', max_iter=None, n_iter=None,\n",
       "                n_jobs=1, random_state=None, shuffle=True, tol=None,\n",
       "                verbose=0, warm_start=False), 0.0],\n",
       " [MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.0],\n",
       " [MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.0],\n",
       " [MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.0],\n",
       " [MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True), 0.0],\n",
       " [RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "          max_iter=None, normalize=False, random_state=None, solver='auto',\n",
       "          tol=0.001), 0.0],\n",
       " [RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "          max_iter=None, normalize=False, random_state=None, solver='auto',\n",
       "          tol=0.001), 0.0],\n",
       " [RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "          max_iter=None, normalize=False, random_state=None, solver='auto',\n",
       "          tol=0.001), 0.0],\n",
       " [RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
       "          max_iter=None, normalize=False, random_state=None, solver='auto',\n",
       "          tol=0.001), 0.0],\n",
       " [RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), class_weight=None, cv=None,\n",
       "           fit_intercept=True, normalize=False, scoring=None), 0.0],\n",
       " [RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), class_weight=None, cv=None,\n",
       "           fit_intercept=True, normalize=False, scoring=None), 0.0],\n",
       " [RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), class_weight=None, cv=None,\n",
       "           fit_intercept=True, normalize=False, scoring=None), 0.0],\n",
       " [RidgeClassifierCV(alphas=(0.1, 1.0, 10.0), class_weight=None, cv=None,\n",
       "           fit_intercept=True, normalize=False, scoring=None), 0.0],\n",
       " [SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "         eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "         learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "         n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "         shuffle=True, tol=None, verbose=0, warm_start=False), 0.0],\n",
       " [SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "         eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "         learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "         n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "         shuffle=True, tol=None, verbose=0, warm_start=False), 0.0],\n",
       " [SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "         eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "         learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "         n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "         shuffle=True, tol=None, verbose=0, warm_start=False), 0.0],\n",
       " [SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "         eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "         learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "         n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "         shuffle=True, tol=None, verbose=0, warm_start=False), 0.0],\n",
       " [BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True), 0.0],\n",
       " [BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True), 0.0],\n",
       " [BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True), 0.0],\n",
       " [BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True), 0.0],\n",
       " [KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "             weights='uniform'), 0.0],\n",
       " [KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "             weights='uniform'), 0.0],\n",
       " [KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "             weights='uniform'), 0.0],\n",
       " [KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "             weights='uniform'), 0.0],\n",
       " [XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "         colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "         max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "         n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "         reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "         silent=True, subsample=1), 0.0],\n",
       " [XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "         colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "         max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "         n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "         reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "         silent=True, subsample=1), 0.0],\n",
       " [XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "         colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "         max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "         n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "         reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "         silent=True, subsample=1), 0.0],\n",
       " [XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "         colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "         max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "         n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "         reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "         silent=True, subsample=1), 0.0],\n",
       " [MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False), 0.0],\n",
       " [MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False), 0.0],\n",
       " [MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False), 0.0],\n",
       " [MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "         beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "         hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "         learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "         nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "         shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "         verbose=False, warm_start=False), 0.0],\n",
       " [LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False), 0.0],\n",
       " [LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False), 0.0],\n",
       " [LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False), 0.0],\n",
       " [LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False), 0.0],\n",
       " [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), 0.3333333333333333],\n",
       " [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), 0.6666666666666666],\n",
       " [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), 0.0],\n",
       " [DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'), 0.3333333333333333],\n",
       " [ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, random_state=None,\n",
       "            splitter='random'), 0.0],\n",
       " [ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, random_state=None,\n",
       "            splitter='random'), 0.6666666666666666],\n",
       " [ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, random_state=None,\n",
       "            splitter='random'), 0.0],\n",
       " [ExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, random_state=None,\n",
       "            splitter='random'), 0.3333333333333333]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_on_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT RUN This"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes on Count Vectors\n",
    "# accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count,valid_y)\n",
    "# print(\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# # Naive Bayes on Word Level TF IDF Vectors\n",
    "# accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf,valid_y)\n",
    "# print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "# accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram,valid_y)\n",
    "# print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# # Naive Bayes on Character Level TF IDF Vectors\n",
    "# accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars,valid_y)\n",
    "# print(\"NB, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "# # Linear Classifier on Count Vectors\n",
    "# accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count,valid_y)\n",
    "# print(\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# # Linear Classifier on Word Level TF IDF Vectors\n",
    "# accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf,valid_y)\n",
    "# print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "# accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram,valid_y)\n",
    "# print(\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# # Linear Classifier on Character Level TF IDF Vectors\n",
    "# accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars,valid_y)\n",
    "# print(\"LR, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "# # SVM on Ngram Level TF IDF Vectors\n",
    "# accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram,valid_y)\n",
    "# print(\"SVM, N-Gram Vectors: \", accuracy)\n",
    "# accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars,valid_y)\n",
    "# print(\"SVM, CharLevel Vectors: \", accuracy)\n",
    "\n",
    "# # RF on Count Vectors\n",
    "# accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count,valid_y)\n",
    "# print(\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# # RF on Word Level TF IDF Vectors\n",
    "# accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf,valid_y)\n",
    "# print(\"RF, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# # Extereme Gradient Boosting on Count Vectors\n",
    "# accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc(),valid_y)\n",
    "# print(\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# # Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc(),valid_y)\n",
    "# print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# # Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "# accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc(),valid_y)\n",
    "# print(\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(lr=0.1), loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return classifier \n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "#accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "#print(\"NN, Ngram Level TF IDF Vectors\",  accuracy)\n",
    "x_train_meta, x_valid_meta,accuracy = return_meta_score(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "X_TRAIN_META.append(x_train_meta)\n",
    "X_VALID_META.append(x_valid_meta)\n",
    "#print(\"NN, Ngram Level TF IDF n gram Vectors\",  accuracy)\n",
    "\n",
    "x_train_meta, x_valid_meta,accuracy = return_meta_score(classifier, xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars, is_neural_net=True)\n",
    "X_TRAIN_META.append(x_train_meta)\n",
    "X_VALID_META.append(x_valid_meta)\n",
    "print(\"NN, Ngram Level TF IDF n gram char Vectors Validation accuracy\",  accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# pd.DataFrame(embedding_matrix).to_pickle(\"pandas_embeddings.pkl\")\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# embedding_matrix = pd.read_pickle(\"pandas_embeddings.pkl\")\n",
    "# embedding_matrix = np.array(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((2000, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.1), loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_cnn()\n",
    "# accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "# print(\"CNN, Word Embeddings\",  accuracy)\n",
    "x_train_meta, x_valid_meta ,accuracy= return_meta_score(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "X_TRAIN_META.append(x_train_meta)\n",
    "X_VALID_META.append(x_valid_meta)\n",
    "print(\"CNN, Word Embeddings Validation accuracy\",  accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_lstm():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((2000, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.1), loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_lstm()\n",
    "# accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "# print(\"RNN-LSTM, Word Embeddings\",  accuracy)\n",
    "x_train_meta, x_valid_meta,accuracy = return_meta_score(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "X_TRAIN_META.append(x_train_meta)\n",
    "X_VALID_META.append(x_valid_meta)\n",
    "print(\"RNN-LSTM, Word Embeddings Validation accuracy\",  accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_gru():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((2000, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.1), loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rnn_gru()\n",
    "# accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "# print(\"RNN-GRU, Word Embeddings\",  accuracy)\n",
    "x_train_meta, x_valid_meta,accuracy = return_meta_score(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "X_TRAIN_META.append(x_train_meta)\n",
    "X_VALID_META.append(x_valid_meta)\n",
    "print(\"RNN-GRU, Word Embeddings Validation accuracy\",  accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_rnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((2000, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.1), loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_bidirectional_rnn()\n",
    "# accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "# print(\"RNN-Bidirectional, Word Embeddings\",  accuracy)\n",
    "x_train_meta, x_valid_meta,accuracy = return_meta_score(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "X_TRAIN_META.append(x_train_meta)\n",
    "X_VALID_META.append(x_valid_meta)\n",
    "print(\"RNN-Bidirectional, Word Embeddings Validation accuracy\",  accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rcnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((2000, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=0.1), loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "classifier = create_rcnn()\n",
    "# accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "# print(\"CNN, Word Embeddings\",  accuracy)\n",
    "x_train_meta, x_valid_meta,accuracy = return_meta_score(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
    "X_TRAIN_META.append(x_train_meta)\n",
    "X_VALID_META.append(x_valid_meta)\n",
    "print(\"CNN, Word Embeddings Validation accuracy\",  accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "accuracy = train_model(xgboost.XGBClassifier(), np.array(X_TRAIN_META).T, train_y, np.array(X_VALID_META).T,valid_y)\n",
    "print(\"Xgb, META SCORE: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clf = xgboost.XGBClassifier()\n",
    "clf.fit(np.array(X_TRAIN_META).T, train_y)\n",
    "# final model training and scoring \n",
    "# Calculation of AUC-ROC\n",
    "train_accuracy = clf.score(np.array(X_TRAIN_META).T,train_y)\n",
    "test_accuracy = clf.score(np.array(X_VALID_META).T,valid_y)\n",
    "\n",
    "y_score = clf.predict(np.array(X_VALID_META).T)\n",
    "AUC_ROC_on_test = roc_auc_score(valid_y, y_score)\n",
    "print(\"Accuracy is: \"+ str(accuracy))\n",
    "print(\"Area under the curve for ROC is: \"+ str(AUC_ROC_on_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X_TRAIN_META).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
